{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from torch.autograd import grad\n",
    "from torch.autograd.functional import hessian\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam, SGD, LBFGS\n",
    "from src.verify import check_MC\n",
    "from src.utils import same_seed, sample_mask\n",
    "from src.nonconvex import create_loss_fn\n",
    "from src.search import create_search_proj_fn, create_search_loss_fn, search\n",
    "torch.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(losses):\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(20, 30), tight_layout=True)\n",
    "    loss    = [loss[0] for loss in losses]\n",
    "    diff    = [loss[1] for loss in losses]\n",
    "    grad_X  = [loss[2] for loss in losses]\n",
    "    hess_X  = [loss[3] for loss in losses]\n",
    "    f_X     = [loss[4] for loss in losses]\n",
    "    f_Z     = [loss[5] for loss in losses]\n",
    "    trans   = [loss[6] for loss in losses]\n",
    "    \n",
    "    ax[0][0].plot(loss)\n",
    "    ax[0][1].plot(diff)\n",
    "    ax[1][0].plot(grad_X)\n",
    "    ax[1][1].plot(trans)\n",
    "    ax[2][0].plot(hess_X)\n",
    "    ax[2][1].plot(f_X, label=r'f(X)')\n",
    "    ax[2][1].plot(f_Z, label=r'f(Z)')\n",
    "\n",
    "    ax[0][0].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[0][0].set_ylabel(r\"$g(X,Z,E)$\", fontsize=30)\n",
    "    ax[0][0].set_title(f\"Total Loss vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[0][0].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[0][0].yaxis.set_tick_params(labelsize=20)\n",
    "    ax[0][0].grid()\n",
    "\n",
    "    ax[0][1].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[0][1].set_ylabel(r\"$f(Z)-f(X)$\", fontsize=30)\n",
    "    ax[0][1].set_title(r\"$f(Z)-f(X)$ vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[0][1].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[0][1].yaxis.set_tick_params(labelsize=20)\n",
    "    ax[0][1].grid()\n",
    "\n",
    "    ax[1][0].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[1][0].set_ylabel(r\"$\\|\\nabla_Xf(X)\\|_F$\", fontsize=30)\n",
    "    ax[1][0].set_title(r\"$\\|\\nabla_Xf(X)\\|_F$ vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[1][0].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[1][0].yaxis.set_tick_params(labelsize=20)\n",
    "    ax[1][0].set_yscale('log')\n",
    "    ax[1][0].grid()\n",
    "\n",
    "    ax[1][1].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[1][1].set_ylabel(r\"$-\\|XX^T-ZZ^T\\|_F$\", fontsize=30)\n",
    "    ax[1][1].set_title(r\"$-\\|XX^T-ZZ^T\\|_F$ vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[1][1].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[1][1].yaxis.set_tick_params(labelsize=20)\n",
    "    ax[1][1].grid()\n",
    "\n",
    "    ax[2][0].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[2][0].set_ylabel(r\"$-\\lambda_{min}(\\nabla^2_Xf(X))$\", fontsize=30)\n",
    "    ax[2][0].set_title(r\"$-\\lambda_{min}(\\nabla^2_Xf(X))$ vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[2][0].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[2][0].yaxis.set_tick_params(labelsize=20)\n",
    "    # ax[2][0].set_yscale('log')\n",
    "    ax[2][0].grid()\n",
    "\n",
    "    ax[2][1].set_xlabel(\"# of GD Updates\", fontsize=30), \n",
    "    ax[2][1].set_ylabel(\"Function Value\", fontsize=30)\n",
    "    ax[2][1].set_title(\"Function Value vs. # of GD Updates\", fontsize=25, fontname='Comic Sans MS')\n",
    "    ax[2][1].xaxis.set_tick_params(labelsize=20)\n",
    "    ax[2][1].yaxis.set_tick_params(labelsize=20)\n",
    "    # ax[2][1].set_yscale('log')\n",
    "    ax[2][1].legend(loc='best')\n",
    "    ax[2][1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_X_Z_e(n, r, p, mu, e_norm, loss_type='MC_max', optim='Adam', iters=100, lr=0.001, min_lr=5e-5, lr_sched='cosine', T=1, trans_bound=(-1e-2, -1e2)):\n",
    "    mask = torch.tensor(sample_mask(n, n, p), dtype=torch.float32)\n",
    "    X = torch.normal(0.0, torch.ones((n, r)))\n",
    "    Z = torch.normal(0.0, torch.ones((n, r)))\n",
    "    # Z = X + 0.6 * gaussian_vector / torch.linalg.norm(gaussian_vector, ord=torch.inf)\n",
    "    # print(X)\n",
    "    # print(Z)\n",
    "    E = torch.zeros((n, n), requires_grad=True)\n",
    "    X.requires_grad_()\n",
    "    Z.requires_grad_()\n",
    "    parameters = [X, E, Z]\n",
    "    loss_fn = create_loss_fn('MC_PSD', mask=mask, Z=0, ZL=0, ZR=0, e=0)\n",
    "    criterion = create_search_loss_fn(loss_fn, loss_type, mask=mask, e=0, Z=0)\n",
    "    proj_fn = create_search_proj_fn('MC', mask=mask, mu=mu, max_norm=e_norm)\n",
    "    proj_fn(*parameters)\n",
    "    losses = search(\n",
    "        parameters,\n",
    "        criterion,\n",
    "        proj_fn,\n",
    "        optim,\n",
    "        iters,\n",
    "        lr,\n",
    "        min_lr,\n",
    "        lr_sched,\n",
    "        T,\n",
    "        trans_bound,\n",
    "    )\n",
    "    \n",
    "    return parameters, mask, losses\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m trans_bound \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1e-1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1e2\u001b[39m)\n\u001b[1;32m     17\u001b[0m same_seed(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m parameters, mask, losses \u001b[39m=\u001b[39m compute_X_Z_e(\n\u001b[1;32m     19\u001b[0m     n, \n\u001b[1;32m     20\u001b[0m     r, \n\u001b[1;32m     21\u001b[0m     p, \n\u001b[1;32m     22\u001b[0m     mu, \n\u001b[1;32m     23\u001b[0m     e_norm, \n\u001b[1;32m     24\u001b[0m     loss_type, \n\u001b[1;32m     25\u001b[0m     optim, \n\u001b[1;32m     26\u001b[0m     iters, \n\u001b[1;32m     27\u001b[0m     lr, \n\u001b[1;32m     28\u001b[0m     min_lr,\n\u001b[1;32m     29\u001b[0m     lr_sched, \n\u001b[1;32m     30\u001b[0m     T, \n\u001b[1;32m     31\u001b[0m     trans_bound\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     33\u001b[0m X, E, Z \u001b[39m=\u001b[39m parameters[\u001b[39m0\u001b[39m], parameters[\u001b[39m1\u001b[39m], parameters[\u001b[39m2\u001b[39m]\n\u001b[1;32m     35\u001b[0m \u001b[39m# print(f\"m = {m}\")\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mcompute_X_Z_e\u001b[0;34m(n, r, p, mu, e_norm, loss_type, optim, iters, lr, min_lr, lr_sched, T, trans_bound)\u001b[0m\n\u001b[1;32m     14\u001b[0m proj_fn \u001b[39m=\u001b[39m create_search_proj_fn(\u001b[39m'\u001b[39m\u001b[39mMC\u001b[39m\u001b[39m'\u001b[39m, mask\u001b[39m=\u001b[39mmask, mu\u001b[39m=\u001b[39mmu, max_norm\u001b[39m=\u001b[39me_norm)\n\u001b[1;32m     15\u001b[0m proj_fn(\u001b[39m*\u001b[39mparameters)\n\u001b[0;32m---> 16\u001b[0m losses \u001b[39m=\u001b[39m search(\n\u001b[1;32m     17\u001b[0m     parameters,\n\u001b[1;32m     18\u001b[0m     criterion,\n\u001b[1;32m     19\u001b[0m     proj_fn,\n\u001b[1;32m     20\u001b[0m     optim,\n\u001b[1;32m     21\u001b[0m     iters,\n\u001b[1;32m     22\u001b[0m     lr,\n\u001b[1;32m     23\u001b[0m     min_lr,\n\u001b[1;32m     24\u001b[0m     lr_sched,\n\u001b[1;32m     25\u001b[0m     T,\n\u001b[1;32m     26\u001b[0m     trans_bound,\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m parameters, mask, losses\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/Lab/matrix-completion/src/search.py:184\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(parameters, criterion, project_fn, optim, iters, lr, min_lr, lr_sched, T, trans_bound)\u001b[0m\n\u001b[1;32m    182\u001b[0m gradients_2 \u001b[39m=\u001b[39m grad(outputs\u001b[39m=\u001b[39mmax_loss, inputs\u001b[39m=\u001b[39mparameters, retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_unused\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    183\u001b[0m optimizer_W\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 184\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    185\u001b[0m \u001b[39m# if clip_grad:\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m#     torch.nn.utils.clip_grad_norm_(parameters, max_norm=1.0, norm_type=2)\u001b[39;00m\n\u001b[1;32m    188\u001b[0m grad_norm_1 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqrt(\u001b[39msum\u001b[39m([\u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m GG \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(GG)\u001b[39m.\u001b[39msquare() \u001b[39mfor\u001b[39;00m GG \u001b[39min\u001b[39;00m gradients_1]))\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    649\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    650\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[1;32m    354\u001b[0m     tensors,\n\u001b[1;32m    355\u001b[0m     grad_tensors_,\n\u001b[1;32m    356\u001b[0m     retain_graph,\n\u001b[1;32m    357\u001b[0m     create_graph,\n\u001b[1;32m    358\u001b[0m     inputs,\n\u001b[1;32m    359\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    360\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    361\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    826\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n           = 5\n",
    "r           = 1\n",
    "p           = 0.5\n",
    "mu          = 2   # [1, n/r]\n",
    "e_norm      = 1e-9\n",
    "# device      = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# coeff       = torch.tensor([1.0, 1.0], requires_grad=True)\n",
    "loss_type   = 'MC_max'\n",
    "optim       = 'Adam'\n",
    "iters       = 10000\n",
    "lr          = 0.0001\n",
    "min_lr      = 5e-5\n",
    "lr_sched    = 'cosine'\n",
    "T           = 1.5\n",
    "trans_bound = (-1e-1, -1e2)\n",
    "\n",
    "same_seed(0)\n",
    "parameters, mask, losses = compute_X_Z_e(\n",
    "    n, \n",
    "    r, \n",
    "    p, \n",
    "    mu, \n",
    "    e_norm, \n",
    "    loss_type, \n",
    "    optim, \n",
    "    iters, \n",
    "    lr, \n",
    "    min_lr,\n",
    "    lr_sched, \n",
    "    T, \n",
    "    trans_bound\n",
    ")\n",
    "X, E, Z = parameters[0], parameters[1], parameters[2]\n",
    "\n",
    "# print(f\"m = {m}\")\n",
    "print(mask.data)\n",
    "print(X.data)\n",
    "print(Z.data)\n",
    "print(E.data)\n",
    "print((X @ X.T).data)\n",
    "print((Z @ Z.T).data)\n",
    "print(torch.linalg.matrix_norm(X - Z, ord='fro'))\n",
    "print(torch.linalg.matrix_norm((X @ X.T) - (Z @ Z.T) + E, ord='fro'))\n",
    "print(torch.linalg.matrix_norm((X @ X.T), ord='fro'))\n",
    "print(torch.linalg.matrix_norm((Z @ Z.T), ord='fro'))\n",
    "print(f\"(diff, grad, hess, trans) = ({losses[-1][1]:.4f}, {losses[-1][2]:.4f}, {losses[-1][3]:.4f}, {losses[-1][6]:.4f})\")\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DSE(search_loops, n, r, p, mu, e_norm, loss_type='A', optim='Adam', iters=100, lr=0.001, min_lr=5e-5, lr_sched='cosine', T=1, trans_bound=(-1e-1, -1e2)):\n",
    "    same_seed(0)\n",
    "    for i in range(search_loops):\n",
    "        print(f\"======= Search Iter {i} =======\")\n",
    "        parameters, mask, losses = compute_X_Z_e(\n",
    "            n, \n",
    "            r, \n",
    "            p, \n",
    "            mu, \n",
    "            e_norm, \n",
    "            loss_type, \n",
    "            optim, \n",
    "            iters, \n",
    "            lr, \n",
    "            min_lr,\n",
    "            lr_sched,\n",
    "            T,\n",
    "            trans_bound\n",
    "        )\n",
    "        X, E, Z = parameters[0], parameters[1], parameters[2] \n",
    "        if len(losses) < iters + 1:\n",
    "            continue\n",
    "        print(mask.data)\n",
    "        print(f\"(TT_loss, diff, grad_X, hess_X, trans) = ({losses[-1][0]:.2e},{losses[-1][1]:.2e},{losses[-1][2]:.2e},{losses[-1][3]:.2e},{losses[-1][6]:.2e})\")\n",
    "        print(f\"{X.data}\")\n",
    "        print(f\"{Z.data}\")\n",
    "        # print(f\"E = {E.data}\")\n",
    "        # print((X @ X.T).data)\n",
    "        # print((Z @ Z.T).data)\n",
    "        flag = check_MC([X.detach().numpy()], mask.numpy(), [Z.detach().numpy()], p, r, mu, PSD=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Search Iter 0 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 1., 1.],\n",
      "        [1., 1., 0., 1., 0.]])\n",
      "(TT_loss, diff, grad_X, hess_X, trans) = (-1.19e+00,-7.87e-01,3.92e-01,3.92e-01,-3.09e+00)\n",
      "tensor([[ 0.5391],\n",
      "        [-0.0749],\n",
      "        [-0.8091],\n",
      "        [-0.1760],\n",
      "        [-0.8091]])\n",
      "tensor([[-1.0153],\n",
      "        [ 0.0912],\n",
      "        [ 0.8695],\n",
      "        [-0.6424],\n",
      "        [-0.7279]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Convex) Distance to ground truth = 2.92e-01 > 1e-6, thus FAILED :(\n",
      "(Non-convex) Distance to ground truth = 2.35e-04 > 1e-6, thus FAILED :(\n",
      "======= Search Iter 1 =======\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.]])\n",
      "(TT_loss, diff, grad_X, hess_X, trans) = (-5.64e-01,-4.69e-01,2.98e-01,2.98e-01,-3.28e+00)\n",
      "tensor([[-0.5741],\n",
      "        [ 0.7398],\n",
      "        [-0.4130],\n",
      "        [ 0.4987],\n",
      "        [-0.2682]])\n",
      "tensor([[-0.8409],\n",
      "        [-0.8347],\n",
      "        [ 0.4565],\n",
      "        [-1.1237],\n",
      "        [-0.5309]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m sear_loops  \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39m# coeff       = (4,0.3)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[39m# device      = 'cuda:0' if torch.cuda.is_available() else 'cpu'\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m DSE(sear_loops, n, r, p, mu, e_norm, loss_type, optim, iters, lr, min_lr, lr_sched, T, trans_bound)\n",
      "Cell \u001b[0;32mIn[5], line 30\u001b[0m, in \u001b[0;36mDSE\u001b[0;34m(search_loops, n, r, p, mu, e_norm, loss_type, optim, iters, lr, min_lr, lr_sched, T, trans_bound)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mZ\u001b[39m.\u001b[39mdata\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[39m# print(f\"E = {E.data}\")\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# print((X @ X.T).data)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# print((Z @ Z.T).data)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m flag \u001b[39m=\u001b[39m check_MC([X\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()], mask\u001b[39m.\u001b[39;49mnumpy(), [Z\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy()], p, r, mu, PSD\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/Lab/matrix-completion/src/verify.py:39\u001b[0m, in \u001b[0;36mcheck_MC\u001b[0;34m(counter_LR, mask, LR, p, r, mu, PSD, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m     criterion \u001b[39m=\u001b[39m create_loss_fn(\u001b[39m'\u001b[39m\u001b[39mMC_PSD\u001b[39m\u001b[39m'\u001b[39m, Z\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, mask\u001b[39m=\u001b[39mmask, ZL\u001b[39m=\u001b[39mZL, ZR\u001b[39m=\u001b[39mZR, e\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     38\u001b[0m proj_fn \u001b[39m=\u001b[39m create_proj_fn(\u001b[39m\"\u001b[39m\u001b[39mMC\u001b[39m\u001b[39m\"\u001b[39m, mu\u001b[39m=\u001b[39mmu)\n\u001b[0;32m---> 39\u001b[0m _, grad_norm, min_eigen \u001b[39m=\u001b[39m sgd(\n\u001b[1;32m     40\u001b[0m     parameters, \n\u001b[1;32m     41\u001b[0m     criterion,\n\u001b[1;32m     42\u001b[0m     proj_fn,\n\u001b[1;32m     43\u001b[0m     optim   \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mSGD\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     44\u001b[0m     iters   \u001b[39m=\u001b[39;49m \u001b[39m20000\u001b[39;49m, \n\u001b[1;32m     45\u001b[0m     lr      \u001b[39m=\u001b[39;49m \u001b[39m1e-2\u001b[39;49m,\n\u001b[1;32m     46\u001b[0m     min_lr  \u001b[39m=\u001b[39;49m \u001b[39m1e-6\u001b[39;49m,\n\u001b[1;32m     47\u001b[0m     lr_sched\u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcosine\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     48\u001b[0m     ret_eig \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     50\u001b[0m counter_Z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(counter_Z)\n\u001b[1;32m     51\u001b[0m Z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(Z)\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/Lab/matrix-completion/src/nonconvex.py:68\u001b[0m, in \u001b[0;36msgd\u001b[0;34m(parameters, criterion, project_fn, optim, iters, lr, min_lr, lr_sched, ret_eig)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(iters):\n\u001b[1;32m     67\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 68\u001b[0m     loss \u001b[39m=\u001b[39m criterion(\u001b[39m*\u001b[39;49mparameters)\n\u001b[1;32m     69\u001b[0m     losses\u001b[39m.\u001b[39mappend((loss\u001b[39m.\u001b[39mitem()))\n\u001b[1;32m     70\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Library/CloudStorage/OneDrive-UniversityofIllinois-Urbana/桌面/Lab/matrix-completion/src/nonconvex.py:13\u001b[0m, in \u001b[0;36mcreate_loss_fn.<locals>.MC_loss_fn_PSD\u001b[0;34m(X, e, Z)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMC_loss_fn_PSD\u001b[39m(X: torch\u001b[39m.\u001b[39mTensor, e: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39me\u001b[39m\u001b[39m'\u001b[39m], Z: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m kwargs[\u001b[39m'\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49msquare(kwargs[\u001b[39m'\u001b[39;49m\u001b[39mmask\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m*\u001b[39;49m (X \u001b[39m@\u001b[39;49m X\u001b[39m.\u001b[39;49mT \u001b[39m-\u001b[39;49m Z \u001b[39m@\u001b[39;49m Z\u001b[39m.\u001b[39;49mT \u001b[39m+\u001b[39;49m e))\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m \u001b[39m4\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n           = 5\n",
    "r           = 1\n",
    "p           = 0.4\n",
    "mu          = 2   # [1, n/r]\n",
    "e_norm      = 1e-18\n",
    "loss_type   = 'MC_max'\n",
    "optim       = 'Adam'\n",
    "iters       = 10000\n",
    "lr          = 2e-4\n",
    "min_lr      = 5e-5\n",
    "lr_sched    = 'cosine'\n",
    "T           = 1.5\n",
    "trans_bound = (-1e-1, -1e2)\n",
    "sear_loops  = 10\n",
    "# coeff       = (4,0.3)\n",
    "# device      = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DSE(sear_loops, n, r, p, mu, e_norm, loss_type, optim, iters, lr, min_lr, lr_sched, T, trans_bound)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
